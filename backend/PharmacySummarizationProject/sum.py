import torch

device = 0 if torch.cuda.is_available() else "cpu"
print(device)
print(torch.cuda.get_device_name(0))

# from langchain.schema import Document
# from langchain.text_splitter import RecursiveCharacterTextSplitter

# # ‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥ text ‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
# text = "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏õ‡∏û‡∏π‡∏î‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡πÉ‡∏î‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏õ‡∏û‡∏π‡∏î‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏∞‡∏≠‡∏∞‡πÑ‡∏£ ‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡πà‡∏∞‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏¥‡∏°‡∏û‡πå‡∏à‡∏∞‡∏ö‡∏≠‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏∞‡∏Ñ‡∏∞‡∏Ñ‡∏∑‡∏≠ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ö‡∏™‡∏¥‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÉ‡∏Ñ‡∏£ ‡πÜ ‡∏Ñ‡πà‡∏∞‡∏Å‡πá‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏´‡∏¢‡∏¥‡∏ö‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏∞‡∏Ñ‡∏∞‡∏û‡∏π‡∏î‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏û‡∏π‡∏î‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏∏‡∏°‡∏ä‡∏ô ‡πÑ‡∏õ‡∏à‡∏ô‡∏Å‡∏£‡∏∞‡∏ó‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠ ‡∏Ñ‡∏∏‡∏ì‡∏Å‡πá‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏±‡∏Å‡∏ñ‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡∏ï‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å‡∏Ñ‡πà‡∏∞ ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡∏¥‡∏î‡∏â‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ß‡πà‡∏≤ ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÅ‡∏•‡∏∞‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞ ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡πà‡∏∞‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á‡∏Ñ‡∏∑‡∏≠ ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏ö‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡∏ô‡∏∞‡∏Æ‡∏∞‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏Ñ‡∏£ ‡∏ô‡∏±‡πà‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏≠‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏ö‡∏≠‡∏Å‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡πà‡∏∞‡πÉ‡∏´‡πâ‡∏ô‡∏≥‡∏ú‡∏π‡πâ‡∏ü‡∏±‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏™‡∏π‡πà‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏ô‡∏±‡πà‡∏ô‡∏Ñ‡∏∑‡∏≠‡∏ö‡∏≠‡∏Å‡∏ú‡∏π‡πâ‡∏ü‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≤‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏™‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏â‡∏≤‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡πÄ‡∏ú‡πá‡∏î‡∏™‡∏õ‡∏µ‡∏Å‡∏™‡∏õ‡∏≤‡∏Ñ‡πÄ‡∏£‡∏≤‡∏Ç‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞‡∏ñ‡∏π‡∏Å‡∏ó‡∏±‡∏ô‡∏Ñ‡πà‡∏∞"

# # ‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô Document
# doc = [Document(page_content=text)]

# # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
# text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
# split_docs = text_splitter.split_documents(doc)

# # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å chain ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
# from langchain.chains.summarize import load_summarize_chain
# chain = load_summarize_chain(llm, chain_type="map_reduce", verbose=True)
# summary = chain.run(split_docs)

# from openai import OpenAI

# client = OpenAI(
#     base_url="http://localhost:1234/v1",
#     api_key="lm-studio"  # default key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LM Studio
# )


# response = client.chat.completions.create(
#     model="typhoon-7b",  # ‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏ô LM Studio
#     messages=[
#         {"role": "system", "content": "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏â‡∏•‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡πÉ‡∏à‡∏î‡∏µ"},
#         {"role": "user", "content": "‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?"}
#     ],
#     temperature=0.7,
#     max_tokens=512
# )

# print(response.choices[0].message.content)


from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain
# ‡∏™‡∏£‡πâ‡∏≤‡∏á chain ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ö‡∏ö map_reduce

from langchain.chat_models import ChatOpenAI
from langchain_community.chat_models import ChatOpenAI
llm = ChatOpenAI(
    model_name="llama-3.2-1b-instruct",  # ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô LM Studio
    temperature=0.7,
    max_tokens=512,
    openai_api_base="http://localhost:1234/v1",  # ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏¢‡∏±‡∏á LM Studio
    openai_api_key="lm-studio",  # LM Studio ‡πÉ‡∏ä‡πâ key ‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡πÑ‡∏î‡πâ (mock key)
)

# chain = load_summarize_chain(llm, chain_type="map_reduce", verbose=True)

# text = "something"
# docs = [Document(page_content=text)]
# print(docs)
# splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
# split_docs = splitter.split_documents(docs)

# summary = chain.run(split_docs)
# print(summary)


# from langchain_core.prompts import PromptTemplate
# map_prompt = """
# ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:
# "{text}"
# """
# map_prompt_template = PromptTemplate(template=map_prompt, input_variables=["text"])

# combine_prompt = """
# ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 3 bullet point.
# ‡πÇ‡∏î‡∏¢ point ‡πÅ‡∏£‡∏Å‡∏Ñ‡∏∑‡∏≠ ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏∞‡πÑ‡∏£
# ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
# ```{text}```

# """
# combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=["text"])

# # Initialize text splitter
# text_splitter = RecursiveCharacterTextSplitter(separators=["\n\n", "\n"], chunk_size=2000, chunk_overlap=200)
# docs =text_splitter.split_documents(docs)

# # Setup and run summarization chain
# summary_chain = load_summarize_chain(
#     llm=llm,
#     chain_type='map_reduce',
#     map_prompt=map_prompt_template,
#     combine_prompt=combine_prompt_template,
#     #verbose=True
# )

# # Run summarization chain and print output
# output = summary_chain.run(docs)
# print(output)

# response = llm.invoke("‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á: ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏â‡∏±‡∏ô‡πÑ‡∏õ‡πÄ‡∏î‡∏¥‡∏ô‡∏ï‡∏•‡∏≤‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏ú‡∏•‡πÑ‡∏°‡πâ‡πÅ‡∏•‡∏∞‡∏Ç‡∏ô‡∏°‡πÑ‡∏ó‡∏¢‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á")
# print(response.content)




#Chatgpt
####################


import pandas as pd
from langchain.docstore.document import Document

# def has_too_many_spaces(text, threshold=0.15):
#     text = text.strip()
#     if len(text) == 0:
#         return True  # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏á ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏ó‡∏¥‡πâ‡∏á
#     space_ratio = text.count(" ") / len(text)
#     return space_ratio > threshold

# filtered_texts = []
# for item in results:
#     text = item['text'].strip()
#     if len(text) > 5 and not has_too_many_spaces(text):
#         filtered_texts.append(text)

df = pd.read_csv("chunk_transcript.csv")
all_text = "\n".join(df["text"].dropna().astype(str))
docs = [Document(page_content=all_text)]
print(docs)

from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain

from langchain.chat_models import ChatOpenAI
from langchain_community.chat_models import ChatOpenAI
llm = ChatOpenAI(
    model_name="llama-3.2-1b-instruct",  # ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô LM Studio
    temperature=0.7,
    max_tokens=512,
    openai_api_base="http://localhost:1234/v1",  # ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏¢‡∏±‡∏á LM Studio
    openai_api_key="lm-studio",  # LM Studio ‡πÉ‡∏ä‡πâ key ‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡πÑ‡∏î‡πâ (mock key)
)
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö map
map_prompt = PromptTemplate.from_template("""
‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢  
‡πÅ‡∏•‡∏∞‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏ô‡∏±‡πâ‡∏ô‡πÅ‡∏ó‡∏ô  
‡πÄ‡∏ä‡πà‡∏ô ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ -> project, ‡πÄ‡∏î‡∏î‡πÑ‡∏• -> deadline ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô:
"{text}"

‡∏™‡∏£‡∏∏‡∏õ:
""")

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° chain ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö map
map_chain = LLMChain(llm=llm, prompt=map_prompt, verbose=True)

# docs = [Document(page_content=text)]
# print(docs)
splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=100)
split_docs = splitter.split_documents(docs)

# ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ
summaries = []
for i, doc in enumerate(split_docs):
    summary = map_chain.run({"text": doc.page_content})
    summaries.append(summary)
    # ‡∏à‡∏∞ print ‡πÅ‡∏•‡∏∞ save ‡∏ó‡∏µ‡∏•‡∏∞ chunk
    print(f"üí° Chunk {i+1} summary:\n{summary}\n")

# ‡∏ï‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô string ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô reduce ‡∏´‡∏£‡∏∑‡∏≠ save
combined_text = "\n\n".join(summaries)

# ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ combine prompt:
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

combine_prompt = PromptTemplate.from_template("""
‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ‡∏¢‡πà‡∏≠‡∏¢‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏¢‡∏≤‡∏ß:

{text}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡∏∏‡∏õ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:
""")

combine_chain = LLMChain(llm=llm, prompt=combine_prompt, verbose=True)
final_summary = combine_chain.run({"text": combined_text})
print(f"\nüîñ Final Summary:\n{final_summary}")


###################